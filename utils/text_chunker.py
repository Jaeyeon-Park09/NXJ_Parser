"""
RAGìš© í…ìŠ¤íŠ¸ í† í° ì²­í¬ ë¶„í•  ëª¨ë“ˆ
OpenAI tiktokenì„ ì‚¬ìš©í•œ ì •í™•í•œ í† í° ê¸°ë°˜ ë¶„í• 
"""

import logging
import re
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from pathlib import Path
from .config_loader import get_config, get_section

# BLIP-2 ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„±ìš© ì¶”ê°€ import
try:
    from transformers import Blip2Processor, Blip2ForConditionalGeneration
    from PIL import Image
    import torch
    BLIP2_AVAILABLE = True
except ImportError:
    BLIP2_AVAILABLE = False

try:
    import tiktoken
except ImportError:
    tiktoken = None
    print("Warning: tiktoken not installed. Install with: pip install tiktoken")

logger = logging.getLogger(__name__)


class TextChunker:
    """í…ìŠ¤íŠ¸ë¥¼ í† í° ê¸°ë°˜ìœ¼ë¡œ ì²­í¬ ë¶„í• í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        TextChunker ì´ˆê¸°í™”
        
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.config = get_config(config_path)
        self.chunk_config = get_section('text_chunking', config_path)
        self.chunk_size = self.chunk_config.get('chunk_size', 500)
        self.overlap = self.chunk_config.get('overlap', 50)
        self.model_name = self.chunk_config.get('model_name', 'gpt-3.5-turbo')
        
        # tiktoken ì¸ì½”ë” ì´ˆê¸°í™”
        if tiktoken:
            try:
                self.encoding = tiktoken.encoding_for_model(self.model_name)
            except KeyError:
                logger.warning(f"Model {self.model_name} not found, using cl100k_base")
                self.encoding = tiktoken.get_encoding("cl100k_base")
        else:
            self.encoding = None
            logger.warning("tiktoken not available, using approximate token counting")
        
        # BLIP-2 ëª¨ë¸ ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
        self.blip2_processor = None
        self.blip2_model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        logger.info(f"í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• ê¸° ì´ˆê¸°í™”: {self.chunk_size} í† í°, {self.overlap} ì˜¤ë²„ë©")
        if BLIP2_AVAILABLE:
            logger.info(f"BLIP-2 ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± ê°€ëŠ¥, ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _load_blip2_model(self):
        """BLIP-2 ëª¨ë¸ ì§€ì—° ë¡œë”© (ìµœì í™”ë¨)"""
        if not BLIP2_AVAILABLE:
            logger.warning("BLIP-2 ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± ë¶ˆê°€")
            return False
            
        if self.blip2_processor is None:
            try:
                # ğŸ”¥ **ì„±ëŠ¥ ìµœì í™”**: ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© ì˜µì…˜
                model_name = self.config.get('image_captioning', {}).get('model', 'Salesforce/blip2-opt-2.7b')
                
                logger.info(f"BLIP-2 ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
                logger.info("â³ ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
                
                # í”„ë¡œì„¸ì„œ ë¡œë“œ (ìºì‹œ í™œìš©)
                self.blip2_processor = Blip2Processor.from_pretrained(
                    model_name,
                    cache_dir=self.config.get('image_captioning', {}).get('cache_dir', None)
                )
                
                # ëª¨ë¸ ë¡œë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)
                self.blip2_model = Blip2ForConditionalGeneration.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                    device_map="auto" if self.device == "cuda" else None,
                    cache_dir=self.config.get('image_captioning', {}).get('cache_dir', None)
                )
                
                if self.device == "cpu":
                    logger.warning("ğŸŒ CPUì—ì„œ BLIP-2 ì‹¤í–‰ ì¤‘ - ì²˜ë¦¬ ì†ë„ê°€ ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
                else:
                    logger.info(f"ğŸš€ GPUì—ì„œ BLIP-2 ì‹¤í–‰: {self.device}")
                
                self.blip2_model.to(self.device)
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
                if self.device == "cuda":
                    allocated = torch.cuda.memory_allocated() / 1024**3
                    logger.info(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {allocated:.1f}GB")
                
                logger.info("âœ… BLIP-2 ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                return True
                
            except Exception as e:
                logger.error(f"âŒ BLIP-2 ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                logger.info("ğŸ’¡ í•´ê²°ì±…: GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ CPU ì‚¬ìš© ë˜ëŠ” ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©")
                return False
        return True
    
    def _generate_image_caption(self, image_path: str) -> Dict[str, Any]:
        """
        BLIP-2ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„±
        
        Args:
            image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ìº¡ì…˜ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        if not self._load_blip2_model():
            return {
                'caption': '',
                'caption_confidence': 0.0,
                'error': 'BLIP-2 ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€'
            }
        
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            if not Path(image_path).exists():
                return {
                    'caption': '',
                    'caption_confidence': 0.0,
                    'error': f'ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ: {image_path}'
                }
            
            image = Image.open(image_path).convert('RGB')
            
            # BLIP-2ë¡œ ìº¡ì…˜ ìƒì„± (ì„¤ì •ê°’ ë°˜ì˜)
            inputs = self.blip2_processor(image, return_tensors="pt").to(self.device)
            
            captioning_config = self.config.get('image_captioning', {})
            max_length = captioning_config.get('max_length', 50)
            temperature = captioning_config.get('temperature', 0.7)
            
            with torch.no_grad():
                generated_ids = self.blip2_model.generate(
                    **inputs, 
                    max_length=max_length,
                    temperature=temperature,
                    do_sample=True if temperature > 0 else False
                )
                caption = self.blip2_processor.batch_decode(
                    generated_ids, skip_special_tokens=True
                )[0].strip()
            
            logger.info(f"ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± ì™„ë£Œ: {image_path} -> '{caption[:50]}...'")
            
            return {
                'caption': caption,
                'caption_confidence': 0.9,  # BLIP-2ëŠ” ì‹ ë¢°ë„ë¥¼ ì§ì ‘ ì œê³µí•˜ì§€ ì•ŠìŒ
                'model_used': 'BLIP-2-OPT-2.7B',
                'generation_timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± ì˜¤ë¥˜ ({image_path}): {e}")
            return {
                'caption': '',
                'caption_confidence': 0.0,
                'error': str(e)
            }
    
    def count_tokens(self, text: str) -> int:
        """
        í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°
        
        Args:
            text: ê³„ì‚°í•  í…ìŠ¤íŠ¸
            
        Returns:
            í† í° ìˆ˜
        """
        if not text:
            return 0
            
        if self.encoding:
            return len(self.encoding.encode(text))
        else:
            # tiktokenì´ ì—†ëŠ” ê²½ìš° ê·¼ì‚¬ì¹˜ ê³„ì‚° (ì˜ì–´: 4ì/í† í°, í•œêµ­ì–´: 2ì/í† í°)
            korean_chars = len(re.findall(r'[ê°€-í£]', text))
            other_chars = len(text) - korean_chars
            return int(korean_chars / 2 + other_chars / 4)
    
    def split_text_by_sentences(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        
        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
            
        Returns:
            ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        """
        # í•œêµ­ì–´ì™€ ì˜ì–´ ë¬¸ì¥ ë¶„í•  íŒ¨í„´
        sentence_patterns = [
            r'[.!?]+\s+',  # ì˜ì–´ ë¬¸ì¥ ë
            r'[.!?ã€‚ï¼ï¼Ÿ]+\s*',  # í•œêµ­ì–´ ë¬¸ì¥ ë
            r'\n\s*\n',  # ë‹¨ë½ êµ¬ë¶„
        ]
        
        sentences = [text]
        for pattern in sentence_patterns:
            new_sentences = []
            for sentence in sentences:
                split_sentences = re.split(pattern, sentence)
                new_sentences.extend([s.strip() for s in split_sentences if s.strip()])
            sentences = new_sentences
        
        return sentences
    
    def create_chunks_with_overlap(self, text: str) -> List[Dict[str, Any]]:
        """
        ì˜¤ë²„ë©ì„ ê³ ë ¤í•œ í† í° ì²­í¬ ìƒì„±
        
        Args:
            text: ì²­í¬ë¡œ ë¶„í• í•  í…ìŠ¤íŠ¸
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ë©”íƒ€ë°ì´í„° í¬í•¨)
        """
        if not text or not text.strip():
            return []
        
        sentences = self.split_text_by_sentences(text)
        chunks = []
        current_chunk = ""
        current_tokens = 0
        chunk_id = 0
        
        i = 0
        while i < len(sentences):
            sentence = sentences[i]
            sentence_tokens = self.count_tokens(sentence)
            
            # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ì„ ì¶”ê°€í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸
            if current_tokens + sentence_tokens <= self.chunk_size:
                if current_chunk:
                    current_chunk += " " + sentence
                else:
                    current_chunk = sentence
                current_tokens += sentence_tokens
                i += 1
            else:
                # í˜„ì¬ ì²­í¬ ì €ì¥ (ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ)
                if current_chunk.strip():
                    chunks.append(self._create_chunk_metadata(
                        current_chunk, chunk_id, current_tokens
                    ))
                    chunk_id += 1
                
                # ì˜¤ë²„ë© ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°±íŠ¸ë˜í‚¹
                overlap_text = ""
                overlap_tokens = 0
                j = i - 1
                
                # ì˜¤ë²„ë© í¬ê¸°ë§Œí¼ ì´ì „ ë¬¸ì¥ë“¤ì„ í¬í•¨
                while j >= 0 and overlap_tokens < self.overlap:
                    prev_sentence = sentences[j]
                    prev_tokens = self.count_tokens(prev_sentence)
                    
                    if overlap_tokens + prev_tokens <= self.overlap:
                        if overlap_text:
                            overlap_text = prev_sentence + " " + overlap_text
                        else:
                            overlap_text = prev_sentence
                        overlap_tokens += prev_tokens
                        j -= 1
                    else:
                        break
                
                # ìƒˆ ì²­í¬ ì‹œì‘ (ì˜¤ë²„ë© í¬í•¨)
                current_chunk = overlap_text
                current_tokens = overlap_tokens
                
                # í˜„ì¬ ë¬¸ì¥ì´ ì²­í¬ í¬ê¸°ë³´ë‹¤ í° ê²½ìš° ê°•ì œ ë¶„í• 
                if sentence_tokens > self.chunk_size:
                    # ê¸´ ë¬¸ì¥ì„ ê°•ì œë¡œ ë¶„í• 
                    words = sentence.split()
                    word_chunk = ""
                    word_tokens = 0
                    
                    for word in words:
                        word_token_count = self.count_tokens(word + " ")
                        if word_tokens + word_token_count <= self.chunk_size - current_tokens:
                            word_chunk += word + " "
                            word_tokens += word_token_count
                        else:
                            # í˜„ì¬ ì›Œë“œë“¤ì„ ì²­í¬ì— ì¶”ê°€
                            if word_chunk.strip():
                                current_chunk += " " + word_chunk.strip() if current_chunk else word_chunk.strip()
                                current_tokens += word_tokens
                            
                            # ì²­í¬ ì €ì¥
                            if current_chunk.strip():
                                chunks.append(self._create_chunk_metadata(
                                    current_chunk, chunk_id, current_tokens
                                ))
                                chunk_id += 1
                            
                            # ìƒˆ ì²­í¬ ì‹œì‘
                            current_chunk = word
                            current_tokens = word_token_count
                            word_chunk = ""
                            word_tokens = 0
                    
                    # ë‚¨ì€ ë‹¨ì–´ë“¤ ì¶”ê°€
                    if word_chunk.strip():
                        current_chunk += " " + word_chunk.strip() if current_chunk else word_chunk.strip()
                        current_tokens += word_tokens
                    
                    i += 1
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk.strip():
            chunks.append(self._create_chunk_metadata(
                current_chunk, chunk_id, current_tokens
            ))
        
        logger.info(f"í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        return chunks
    
    def _create_chunk_metadata(self, text: str, chunk_id: int, token_count: int) -> Dict[str, Any]:
        """
        ì²­í¬ ë©”íƒ€ë°ì´í„° ìƒì„±
        
        Args:
            text: ì²­í¬ í…ìŠ¤íŠ¸
            chunk_id: ì²­í¬ ID
            token_count: í† í° ìˆ˜
            
        Returns:
            ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ ì²­í¬ ë”•ì…”ë„ˆë¦¬
        """
        return {
            "chunk_id": chunk_id,
            "text": text.strip(),
            "token_count": token_count,
            "character_count": len(text.strip()),
            "word_count": len(text.strip().split()),
            "created_at": datetime.now().isoformat(),
            "chunk_size_limit": self.chunk_size,
            "overlap_size": self.overlap
        }
    
    def chunk_document_content(self, document_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        BLIP-2 + GMFT + OCRì„ í™œìš©í•œ ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²­í¬ ë¶„í• 
        
        Args:
            document_data: íŒŒì‹±ëœ ë¬¸ì„œ ë°ì´í„°  
            
        Returns:
            ë¶„ë¦¬ëœ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, í‘œê°€ í¬í•¨ëœ ë¬¸ì„œ ë°ì´í„°
        """
        try:
            # ì „ì²´ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ì—ì„œ ì²­í¬ ìƒì„±
            full_markdown = document_data.get('full_markdown', '')
            
            if not full_markdown:
                logger.warning("ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                self._create_empty_multimodal_structure(document_data)
                return document_data
            
            # í—¤ë” ë¶€ë¶„ ì œê±° (--- ì´í›„ë¶€í„° ì²­í¬ ë¶„í• )
            if '---' in full_markdown:
                content_parts = full_markdown.split('---', 1)
                main_content = content_parts[1].strip() if len(content_parts) > 1 else full_markdown
            else:
                main_content = full_markdown
            
            # ê¸°ë³¸ ì²­í¬ ìƒì„± (í…ìŠ¤íŠ¸ + ë¯¸ë””ì–´ ë§ˆí¬ë‹¤ìš´ í¬í•¨)
            raw_chunks = self.create_chunks_with_overlap(main_content)
            
            # ğŸ”¥ **í•µì‹¬: ë©€í‹°ëª¨ë‹¬ ë¯¸ë””ì–´ ë¶„ë¦¬ ë° ê°•í™”**
            media_registry = self._process_multimodal_content(raw_chunks, document_data)
            
            # ë¶„ë¦¬ëœ êµ¬ì¡°ë¡œ ë¬¸ì„œ ë°ì´í„° êµ¬ì„±
            document_data.update({
                'text_chunks': media_registry['text_only_chunks'],      # ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ
                'contextual_images': media_registry['images'],         # BLIP-2 ìº¡ì…˜ + ì»¨í…ìŠ¤íŠ¸
                'contextual_tables': media_registry['tables'],         # GMFT êµ¬ì¡° + OCR/í…ìŠ¤íŠ¸
                'media_registry': media_registry,                      # ì „ì²´ ë¯¸ë””ì–´ ë ˆì§€ìŠ¤íŠ¸ë¦¬
                'multimodal_processing_stats': media_registry['processing_stats']
            })
            
            # ğŸ”¥ **ë¶„ë¦¬ëœ ë©€í‹°ëª¨ë‹¬ êµ¬ì¡° ê¸°ë°˜ í†µê³„ ìƒì„±**
            text_chunks = media_registry['text_only_chunks']
            chunk_stats = {
                "total_text_chunks": len(text_chunks),
                "total_chunk_tokens": sum(chunk['token_count'] for chunk in text_chunks),
                "average_chunk_tokens": sum(chunk['token_count'] for chunk in text_chunks) / len(text_chunks) if text_chunks else 0,
                "chunk_size_limit": self.chunk_size,
                "overlap_size": self.overlap,
                "processing_timestamp": datetime.now().isoformat(),
                
                # **ë¶„ë¦¬ëœ ë©€í‹°ëª¨ë‹¬ í†µê³„** 
                "separated_multimodal_stats": {
                    "total_contextual_images": len(media_registry['images']),
                    "images_with_blip2_captions": media_registry['processing_stats']['images_with_blip2_captions'],
                    "images_with_ocr": media_registry['processing_stats']['images_with_ocr'],
                    "total_contextual_tables": len(media_registry['tables']),
                    "tables_with_content": media_registry['processing_stats']['tables_with_content'],
                    "text_chunks_with_media_refs": media_registry['processing_stats']['chunks_with_media_references'],
                    "processing_approach": "BLIP-2_caption + GMFT_structure + OCR_extraction"
                }
            }
            
            # ê¸°ì¡´ í†µê³„ì— ì²­í¬ ì •ë³´ ì¶”ê°€
            if 'document_stats' not in document_data:
                document_data['document_stats'] = {}
            
            document_data['document_stats']['chunking'] = chunk_stats
            
            logger.info(f"ğŸ”¥ BLIP-2 + GMFT ë©€í‹°ëª¨ë‹¬ ë¶„í•  ì™„ë£Œ: "
                       f"{len(text_chunks)}ê°œ í…ìŠ¤íŠ¸ ì²­í¬ (í‰ê·  {chunk_stats['average_chunk_tokens']:.1f} í† í°), "
                       f"{len(media_registry['images'])}ê°œ ì´ë¯¸ì§€ "
                       f"(BLIP-2 ìº¡ì…˜ {media_registry['processing_stats']['images_with_blip2_captions']}ê°œ), "
                       f"{len(media_registry['tables'])}ê°œ í‘œ")
            
            return document_data
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì²­í¬ ë¶„í•  ì˜¤ë¥˜: {str(e)}")
            document_data['text_chunks'] = []
            return document_data
    
    def _add_multimodal_metadata(self, chunks: List[Dict[str, Any]], 
                                document_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ì²­í¬ì— ì´ë¯¸ì§€/í‘œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        
        Args:
            chunks: ì›ë³¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
            document_data: ì›ë³¸ ë¬¸ì„œ ë°ì´í„°
            
        Returns:
            ë©”íƒ€ë°ì´í„°ê°€ ì¶”ê°€ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        try:
            blocks = document_data.get('blocks', [])
            image_blocks = [b for b in blocks if b.get('type') == 'image_ocr']
            table_blocks = [b for b in blocks if b.get('type') == 'table']
            
            enhanced_chunks = []
            
            for chunk in chunks:
                chunk_text = chunk.get('text', '')
                
                # ì²­í¬ì— í¬í•¨ëœ ì´ë¯¸ì§€ ì°¾ê¸°
                chunk_images = []
                for img_block in image_blocks:
                    img_content = img_block.get('content', '')
                    img_path = img_block.get('source', {}).get('image_path', '')
                    
                    # ì´ë¯¸ì§€ ê²½ë¡œê°€ ì²­í¬ í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    if img_path and img_path in chunk_text:
                        chunk_images.append({
                            'image_path': img_path,
                            'ocr_text': img_content,
                            'page': img_block.get('source', {}).get('page', 0),
                            'bbox': img_block.get('source', {}).get('bbox', []),
                            'confidence': img_block.get('metadata', {}).get('ocr_confidence', 0)
                        })
                
                # ì²­í¬ì— í¬í•¨ëœ í‘œ ì°¾ê¸° (ë§ˆí¬ë‹¤ìš´ í‘œ íŒ¨í„´ë„ ê°ì§€)
                chunk_tables = []
                
                # GMFT/ê¸°íƒ€ ê°ì§€ëœ í‘œ ë¸”ë¡
                for table_block in table_blocks:
                    table_content = table_block.get('content', '')
                    table_id = table_block.get('metadata', {}).get('table_id', '')
                    
                    if table_content and (table_content[:100] in chunk_text or table_id in chunk_text):
                        chunk_tables.append({
                            'table_id': table_id,
                            'type': 'structured_table',
                            'markdown_content': table_content,
                            'page': table_block.get('source', {}).get('page', 0),
                            'bbox': table_block.get('source', {}).get('bbox', []),
                            'dimensions': table_block.get('metadata', {}).get('dimensions', {}),
                            'confidence': table_block.get('metadata', {}).get('confidence', 1.0)
                        })
                
                # ë§ˆí¬ë‹¤ìš´ í‘œ íŒ¨í„´ ê°ì§€ (Markerê°€ ë³€í™˜í•œ í‘œë“¤)
                import re
                markdown_table_pattern = r'\|[^|\n]*\|[^|\n]*\|'
                table_matches = re.findall(markdown_table_pattern, chunk_text)
                
                for i, table_match in enumerate(table_matches):
                    # ë§ˆí¬ë‹¤ìš´ í‘œì˜ ì¤„ ìˆ˜ ê³„ì‚°
                    table_lines = [line for line in table_match.split('\n') if '|' in line and '---' not in line]
                    if len(table_lines) >= 2:  # í—¤ë” + ìµœì†Œ 1ê°œ ë°ì´í„° í–‰
                        chunk_tables.append({
                            'table_id': f'markdown_table_{chunk["chunk_id"]}_{i}',
                            'type': 'markdown_table',
                            'markdown_content': table_match,
                            'estimated_rows': len(table_lines) - 1,  # í—¤ë” ì œì™¸
                            'estimated_cols': len(table_lines[0].split('|')) - 2 if table_lines else 0,
                            'extraction_method': 'marker_text_based'
                        })
                
                # ì²­í¬ì— ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                enhanced_chunk = chunk.copy()
                enhanced_chunk['multimodal_content'] = {
                    'has_images': len(chunk_images) > 0,
                    'has_tables': len(chunk_tables) > 0,
                    'images': chunk_images,
                    'tables': chunk_tables,
                    'total_media_items': len(chunk_images) + len(chunk_tables)
                }
                
                enhanced_chunks.append(enhanced_chunk)
            
            logger.info(f"ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€ ì™„ë£Œ: "
                       f"{sum(len(c['multimodal_content']['images']) for c in enhanced_chunks)}ê°œ ì´ë¯¸ì§€, "
                       f"{sum(len(c['multimodal_content']['tables']) for c in enhanced_chunks)}ê°œ í‘œ")
            
            return enhanced_chunks
            
        except Exception as e:
            logger.error(f"ë©€í‹°ëª¨ë‹¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€ ì˜¤ë¥˜: {str(e)}")
            return chunks  # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ì²­í¬ ë°˜í™˜
    
    def get_chunk_statistics(self, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ì²­í¬ í†µê³„ ì •ë³´ ìƒì„±
        
        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            í†µê³„ ì •ë³´
        """
        if not chunks:
            return {
                "total_chunks": 0,
                "total_tokens": 0,
                "average_tokens": 0,
                "min_tokens": 0,
                "max_tokens": 0
            }
        
        token_counts = [chunk['token_count'] for chunk in chunks]
        
        return {
            "total_chunks": len(chunks),
            "total_tokens": sum(token_counts),
            "average_tokens": sum(token_counts) / len(token_counts),
            "min_tokens": min(token_counts),
            "max_tokens": max(token_counts),
            "chunk_size_limit": self.chunk_size,
            "overlap_size": self.overlap
        } 

    def _create_empty_multimodal_structure(self, document_data: Dict[str, Any]):
        """ë¹ˆ ë©€í‹°ëª¨ë‹¬ êµ¬ì¡° ìƒì„±"""
        document_data.update({
            'text_chunks': [],
            'contextual_images': [],
            'contextual_tables': [],
            'media_registry': {
                'images': [], 'tables': [], 'text_only_chunks': [],
                'processing_stats': {'error': 'ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ ì—†ìŒ'}
            },
            'multimodal_processing_stats': {'error': 'ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ ì—†ìŒ'}
        })
    
    def _process_multimodal_content(self, raw_chunks: List[Dict[str, Any]], 
                                   document_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ”¥ **í•µì‹¬ í•¨ìˆ˜**: BLIP-2 + GMFT + OCRì„ í™œìš©í•œ ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬
        
        Args:
            raw_chunks: ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²­í¬ë“¤
            document_data: ë¬¸ì„œ ë°ì´í„°
            
        Returns:
            ì²˜ë¦¬ëœ ë¯¸ë””ì–´ ë ˆì§€ìŠ¤íŠ¸ë¦¬
        """
        try:
            blocks = document_data.get('blocks', [])
            full_markdown = document_data.get('full_markdown', '')
            
            media_registry = {
                'images': [],
                'tables': [],
                'text_only_chunks': [],
                'processing_stats': {
                    'total_images_processed': 0,
                    'images_with_blip2_captions': 0,
                    'images_with_ocr': 0,
                    'total_tables_processed': 0,
                    'tables_with_gmft_structure': 0,
                    'tables_with_content': 0,
                    'text_chunks_created': 0,
                    'chunks_with_media_references': 0
                }
            }
            
            # ğŸ–¼ï¸ **1ë‹¨ê³„: ì´ë¯¸ì§€ ì²˜ë¦¬ (BLIP-2 ìº¡ì…˜ + OCR + ì»¨í…ìŠ¤íŠ¸)**
            self._process_images_with_blip2(blocks, media_registry, full_markdown)
            
            # ğŸ“Š **2ë‹¨ê³„: í‘œ ì²˜ë¦¬ (GMFT êµ¬ì¡° + OCR/í…ìŠ¤íŠ¸ ì¶”ì¶œ)**
            self._process_tables_with_gmft(blocks, document_data, media_registry, full_markdown)
            
            # ğŸ“ **3ë‹¨ê³„: í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„± (ë¯¸ë””ì–´ ì œê±° + ì°¸ì¡° ìœ ì§€)**
            self._create_text_only_chunks(raw_chunks, media_registry)
            
            logger.info(f"ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ì™„ë£Œ: "
                       f"ì´ë¯¸ì§€ {media_registry['processing_stats']['total_images_processed']}ê°œ "
                       f"(BLIP-2 ìº¡ì…˜ {media_registry['processing_stats']['images_with_blip2_captions']}ê°œ), "
                       f"í‘œ {media_registry['processing_stats']['total_tables_processed']}ê°œ, "
                       f"í…ìŠ¤íŠ¸ ì²­í¬ {media_registry['processing_stats']['text_chunks_created']}ê°œ")
            
            return media_registry
            
        except Exception as e:
            logger.error(f"ë©€í‹°ëª¨ë‹¬ ì½˜í…ì¸  ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return {
                'images': [], 'tables': [], 'text_only_chunks': raw_chunks,
                'processing_stats': {'error': str(e)}
            }
    
    def _process_images_with_blip2(self, blocks: List[Dict], media_registry: Dict, full_markdown: str):
        """BLIP-2ë¥¼ í™œìš©í•œ ì´ë¯¸ì§€ ì²˜ë¦¬"""
        image_blocks = [b for b in blocks if b.get('type') == 'image_ocr']
        
        for img_block in image_blocks:
            page = img_block.get('source', {}).get('page', 0)
            image_path = img_block.get('source', {}).get('image_path', '')
            ocr_content = img_block.get('content', '')
            
            # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
            context = self._extract_media_context(blocks, img_block)
            
            # ğŸ”¥ BLIP-2 ìº¡ì…˜ ìƒì„± (í•µì‹¬!)
            captioning_enabled = self.config.get('image_captioning', {}).get('enabled', True)
            
            if captioning_enabled and image_path:
                caption_info = self._generate_image_caption(image_path)
            else:
                reason = 'BLIP-2 disabled' if not captioning_enabled else 'No image path'
                caption_info = {
                    'caption': '', 
                    'caption_confidence': 0.0, 
                    'error': reason
                }
            
            image_item = {
                'media_id': f'img_{page}_{len(media_registry["images"])}',
                'type': 'image',
                'image_path': image_path,
                'page': page,
                'bbox': img_block.get('source', {}).get('bbox', []),
                
                # ğŸ¯ **RAGìš© ì˜ë¯¸ìˆëŠ” ì½˜í…ì¸ **
                'content_analysis': {
                    'ocr_text': ocr_content,
                    'ocr_confidence': img_block.get('metadata', {}).get('ocr_confidence', 0),
                    'blip2_caption': caption_info.get('caption', ''),
                    'caption_confidence': caption_info.get('caption_confidence', 0),
                    'combined_description': self._combine_ocr_and_caption(ocr_content, caption_info.get('caption', '')),
                    'has_meaningful_content': bool(ocr_content or caption_info.get('caption', ''))
                },
                
                # ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ (ê²€ìƒ‰ ì‹œ í™œìš©)
                'context': context,
                'text_references': self._find_image_references(full_markdown, image_path)
            }
            
            media_registry['images'].append(image_item)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            media_registry['processing_stats']['total_images_processed'] += 1
            if caption_info.get('caption'):
                media_registry['processing_stats']['images_with_blip2_captions'] += 1
            if ocr_content:
                media_registry['processing_stats']['images_with_ocr'] += 1
    
    def _process_tables_with_gmft(self, blocks: List[Dict], document_data: Dict, 
                                 media_registry: Dict, full_markdown: str):
        """GMFT + OCR ì¡°í•©ì„ í™œìš©í•œ í‘œ ì²˜ë¦¬"""
        
        # ê¸°ì¡´ Marker ê°ì§€ í‘œ ì²˜ë¦¬ (ë²¡í„° ê¸°ë°˜)
        table_blocks = [b for b in blocks if b.get('type') == 'table']
        for table_block in table_blocks:
            page = table_block.get('source', {}).get('page', 0)
            context = self._extract_media_context(blocks, table_block)
            
            table_item = {
                'media_id': f'table_{page}_{len(media_registry["tables"])}',
                'type': 'table',
                'detection_method': 'marker_vector_based',
                'page': page,
                'bbox': table_block.get('source', {}).get('bbox', []),
                
                # í‘œ ì½˜í…ì¸  ë¶„ì„
                'content_analysis': {
                    'markdown_content': table_block.get('content', ''),
                    'content_source': 'pdf_vector_text',  # ë²¡í„° ê¸°ë°˜ í…ìŠ¤íŠ¸
                    'row_count': self._count_markdown_table_rows(table_block.get('content', '')),
                    'col_count': self._count_markdown_table_cols(table_block.get('content', '')),
                    'has_meaningful_content': bool(table_block.get('content', '').strip())
                },
                
                'context': context,
                'text_references': self._find_table_references(full_markdown)
            }
            
            media_registry['tables'].append(table_item)
            media_registry['processing_stats']['total_tables_processed'] += 1
            if table_block.get('content', '').strip():
                media_registry['processing_stats']['tables_with_content'] += 1
        
        # TODO: GMFT ê°ì§€ í‘œ ì¶”ê°€ ì²˜ë¦¬ (ì´ë¯¸ì§€ ê¸°ë°˜ - ë” ë³µì¡í•œ êµ¬í˜„ í•„ìš”)
        # gmft_tables = self._get_gmft_detected_tables(document_data)
        # for gmft_table in gmft_tables:
        #     self._process_gmft_table_with_ocr(gmft_table, media_registry)
    
    def _create_text_only_chunks(self, raw_chunks: List[Dict], media_registry: Dict):
        """ë¯¸ë””ì–´ ì œê±°ëœ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±"""
        for chunk in raw_chunks:
            original_text = chunk.get('text', '')
            
            # ë¯¸ë””ì–´ ë§ˆí¬ë‹¤ìš´ ì œê±°í•œ ìˆœìˆ˜ í…ìŠ¤íŠ¸
            clean_text = self._remove_media_markdown(original_text)
            
            if clean_text.strip():
                text_chunk = chunk.copy()
                text_chunk['text'] = clean_text
                text_chunk['is_text_only'] = True
                
                # ğŸ”— ë¯¸ë””ì–´ ì°¸ì¡° ì—°ê²° (RAG ê²€ìƒ‰ ì‹œ ì¤‘ìš”!)
                referenced_media = self._find_chunk_media_references(original_text, media_registry)
                text_chunk['referenced_media'] = referenced_media
                
                media_registry['text_only_chunks'].append(text_chunk)
                media_registry['processing_stats']['text_chunks_created'] += 1
                
                if referenced_media:
                    media_registry['processing_stats']['chunks_with_media_references'] += 1 

    # =============== í—¬í¼ í•¨ìˆ˜ë“¤ ===============
    
    def _extract_media_context(self, blocks: List[Dict], target_block: Dict) -> Dict[str, str]:
        """ë¯¸ë””ì–´ ë¸”ë¡ ì£¼ë³€ì˜ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        target_page = target_block.get('source', {}).get('page', 0)
        target_bbox = target_block.get('source', {}).get('bbox', [])
        
        if not target_bbox or len(target_bbox) < 4:
            return {
                'preceding_text': '',
                'following_text': '',
                'page_title': '',
                'section_header': ''
            }
        
        target_y = target_bbox[1]  # y ì¢Œí‘œ
        
        preceding_texts = []
        following_texts = []
        
        # ê°™ì€ í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ë¸”ë¡ë“¤ì—ì„œ ì£¼ë³€ í…ìŠ¤íŠ¸ ì°¾ê¸°
        for block in blocks:
            if (block.get('source', {}).get('page', -1) == target_page and 
                block.get('type') in ['Text', 'Span', 'Line', 'text', 'paragraph'] and
                block.get('source', {}).get('bbox')):
                
                block_bbox = block.get('source', {}).get('bbox', [])
                if len(block_bbox) >= 4:
                    block_y = block_bbox[1]
                    content = block.get('content', '').strip()
                    
                    if content:
                        if block_y < target_y:  # íƒ€ê²Ÿë³´ë‹¤ ìœ„ì— ìˆëŠ” í…ìŠ¤íŠ¸
                            preceding_texts.append((block_y, content))
                        elif block_y > target_y:  # íƒ€ê²Ÿë³´ë‹¤ ì•„ë˜ì— ìˆëŠ” í…ìŠ¤íŠ¸
                            following_texts.append((block_y, content))
        
        # ê±°ë¦¬ìˆœ ì •ë ¬ í›„ ê°€ì¥ ê°€ê¹Œìš´ í…ìŠ¤íŠ¸ë“¤ ì„ íƒ
        preceding_texts.sort(key=lambda x: x[0], reverse=True)  # ê°€ê¹Œìš´ ìˆœìœ¼ë¡œ
        following_texts.sort(key=lambda x: x[0])  # ê°€ê¹Œìš´ ìˆœìœ¼ë¡œ
        
        return {
            'preceding_text': ' '.join([text for _, text in preceding_texts[:3]]),  # ì• 3ê°œ
            'following_text': ' '.join([text for _, text in following_texts[:3]]),   # ë’¤ 3ê°œ
            'page_title': self._extract_page_title(blocks, target_page),
            'section_header': self._extract_nearest_section_header(blocks, target_block)
        }
    
    def _extract_page_title(self, blocks: List[Dict], page: int) -> str:
        """í˜ì´ì§€ì˜ ì œëª©/í—¤ë” ì¶”ì¶œ"""
        for block in blocks:
            if (block.get('source', {}).get('page', -1) == page and 
                block.get('type') in ['SectionHeader', 'title', 'header']):
                return block.get('content', '').strip()
        return ''
    
    def _extract_nearest_section_header(self, blocks: List[Dict], target_block: Dict) -> str:
        """íƒ€ê²Ÿ ë¸”ë¡ì— ê°€ì¥ ê°€ê¹Œìš´ ì„¹ì…˜ í—¤ë” ì°¾ê¸°"""
        target_page = target_block.get('source', {}).get('page', 0)
        target_y = target_block.get('source', {}).get('bbox', [0, 0])[1]
        
        closest_header = ''
        min_distance = float('inf')
        
        for block in blocks:
            if (block.get('source', {}).get('page', -1) == target_page and 
                block.get('type') == 'SectionHeader'):
                
                block_bbox = block.get('source', {}).get('bbox', [])
                if len(block_bbox) >= 4:
                    block_y = block_bbox[1]
                    if block_y < target_y:  # íƒ€ê²Ÿë³´ë‹¤ ìœ„ì— ìˆëŠ” í—¤ë”
                        distance = target_y - block_y
                        if distance < min_distance:
                            min_distance = distance
                            closest_header = block.get('content', '').strip()
        
        return closest_header
    
    def _combine_ocr_and_caption(self, ocr_text: str, blip2_caption: str) -> str:
        """OCR í…ìŠ¤íŠ¸ì™€ BLIP-2 ìº¡ì…˜ì„ ì¡°í•©í•œ ì™„ì „í•œ ì´ë¯¸ì§€ ì„¤ëª…"""
        combined_parts = []
        
        if blip2_caption.strip():
            combined_parts.append(f"Image description: {blip2_caption}")
        
        if ocr_text.strip():
            combined_parts.append(f"Text in image: {ocr_text}")
        
        return ' | '.join(combined_parts) if combined_parts else ''
    
    def _find_image_references(self, full_markdown: str, image_path: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ì§€ ì°¸ì¡° í‘œí˜„ ì°¾ê¸°"""
        import re
        references = []
        
        if image_path:
            filename = Path(image_path).stem  # íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)
            
            # í•œêµ­ì–´ ì°¸ì¡° í‘œí˜„ íŒ¨í„´
            korean_patterns = [
                r'ìœ„\s*(?:ê·¸ë¦¼|ì´ë¯¸ì§€|ì‚¬ì§„|ë„í‘œ)',
                r'ì•„ë˜\s*(?:ê·¸ë¦¼|ì´ë¯¸ì§€|ì‚¬ì§„|ë„í‘œ)',
                r'ë‹¤ìŒ\s*(?:ê·¸ë¦¼|ì´ë¯¸ì§€|ì‚¬ì§„|ë„í‘œ)',
                r'ì•\s*(?:ê·¸ë¦¼|ì´ë¯¸ì§€|ì‚¬ì§„|ë„í‘œ)',
                r'(?:ê·¸ë¦¼|ì´ë¯¸ì§€|ì‚¬ì§„|ë„í‘œ)\s*[\d\-_]+',
                filename  # íŒŒì¼ëª… ê¸°ë°˜
            ]
            
            for pattern in korean_patterns:
                matches = re.findall(pattern, full_markdown, re.IGNORECASE)
                references.extend(matches)
        
        return list(set(references))  # ì¤‘ë³µ ì œê±°
    
    def _find_table_references(self, full_markdown: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‘œ ì°¸ì¡° í‘œí˜„ ì°¾ê¸°"""
        import re
        
        table_patterns = [
            r'ìœ„\s*í‘œ',
            r'ì•„ë˜\s*í‘œ',
            r'ë‹¤ìŒ\s*í‘œ',
            r'ì•\s*í‘œ',
            r'í‘œ\s*[\d\-_]+',
            r'ë‹¤ìŒê³¼\s*ê°™ë‹¤',
            r'ì•„ë˜ì™€\s*ê°™ë‹¤'
        ]
        
        references = []
        for pattern in table_patterns:
            matches = re.findall(pattern, full_markdown, re.IGNORECASE)
            references.extend(matches)
        
        return list(set(references))
    
    def _remove_media_markdown(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ì§€/í‘œ ë§ˆí¬ë‹¤ìš´ ì œê±°"""
        import re
        
        # ì´ë¯¸ì§€ ë§ˆí¬ë‹¤ìš´ ì œê±°: ![ì´ë¯¸ì§€](path)
        text = re.sub(r'!\[.*?\]\([^)]*\)', '[ì´ë¯¸ì§€ ì œê±°ë¨]', text)
        
        # ì´ë¯¸ì§€ ì„¤ëª… ì„¹ì…˜ ì œê±°: **ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸:** > ...
        text = re.sub(r'\*\*ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸:\*\*[^\n]*\n?', '', text)
        text = re.sub(r'>\s*[^\n]*\n?', '', text)  # > ë¡œ ì‹œì‘í•˜ëŠ” ì¸ìš© í…ìŠ¤íŠ¸
        
        # í‘œ ë§ˆí¬ë‹¤ìš´ ì œê±°: | col1 | col2 |
        text = re.sub(r'\|[^|\n]*\|[^|\n]*\|[^\n]*\n?', '[í‘œ ì œê±°ë¨]\n', text)
        text = re.sub(r'\|[-\s]*\|[-\s]*\|[^\n]*\n?', '', text)  # í‘œ êµ¬ë¶„ì„  ì œê±°
        
        # ì—°ì†ëœ ë¹ˆ ì¤„ê³¼ ì œê±°ë¨ í‘œì‹œ ì •ë¦¬
        text = re.sub(r'\[ì´ë¯¸ì§€ ì œê±°ë¨\]\s*\[í‘œ ì œê±°ë¨\]', '[ë¯¸ë””ì–´ ì œê±°ë¨]', text)
        text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
        
        return text.strip()
    
    def _find_chunk_media_references(self, original_text: str, media_registry: Dict) -> List[str]:
        """ì²­í¬ í…ìŠ¤íŠ¸ì—ì„œ ì°¸ì¡°í•˜ëŠ” ë¯¸ë””ì–´ IDë“¤ ì°¾ê¸°"""
        referenced_ids = []
        
        # ì´ë¯¸ì§€ ì°¸ì¡° ì°¾ê¸°
        for img in media_registry['images']:
            img_path = img.get('image_path', '')
            if img_path and img_path in original_text:
                referenced_ids.append(img['media_id'])
            
            # í…ìŠ¤íŠ¸ ì°¸ì¡°ë¡œë„ í™•ì¸
            for ref in img.get('text_references', []):
                if ref and ref.lower() in original_text.lower():
                    referenced_ids.append(img['media_id'])
                    break
        
        # í‘œ ì°¸ì¡° ì°¾ê¸°
        for table in media_registry['tables']:
            for ref in table.get('text_references', []):
                if ref and ref.lower() in original_text.lower():
                    referenced_ids.append(table['media_id'])
                    break
        
        return list(set(referenced_ids))  # ì¤‘ë³µ ì œê±°
    
    def _count_markdown_table_rows(self, markdown_content: str) -> int:
        """ë§ˆí¬ë‹¤ìš´ í‘œì˜ í–‰ ìˆ˜ ê³„ì‚°"""
        if not markdown_content:
            return 0
        lines = [line.strip() for line in markdown_content.split('\n') if line.strip()]
        table_lines = [line for line in lines if '|' in line and '---' not in line]
        return max(0, len(table_lines) - 1)  # í—¤ë” ì œì™¸
    
    def _count_markdown_table_cols(self, markdown_content: str) -> int:
        """ë§ˆí¬ë‹¤ìš´ í‘œì˜ ì—´ ìˆ˜ ê³„ì‚°"""
        if not markdown_content:
            return 0
        lines = [line.strip() for line in markdown_content.split('\n') if line.strip()]
        table_lines = [line for line in lines if '|' in line and '---' not in line]
        if table_lines:
            return len(table_lines[0].split('|')) - 2  # ì–‘ìª½ ë¹ˆ ë¶€ë¶„ ì œì™¸
        return 0 